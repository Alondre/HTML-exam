<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Milestones in computer tech</title>
    <h1>Alondre' Jenkins</h1>  
</head>
<body>
  <color:Blue><h2>Apple’s Macintosh</h2></color:Blue>
  <img src="https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.mac-history.net%2Ftop%2F2020-02-10%2Fthe-history-of-the-apple-macintosh&psig=AOvVaw3np2M8MBlzT0AWOJ9JYmLQ&ust=1612243344046000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCKDt5s-FyO4CFQAAAAAdAAAAABAD" alt="height:200">
  <p>The <i>Apple Macintosh</i> Macintosh models were high in price, hindering competitiveness in a market dominated by the much cheaper Commodore 64 for consumers, as well as the IBM Personal Computer and its accompanying clone market for businesses,[4] although they were less expensive than the Xerox Alto and other computers with graphical user interfaces that predated the Mac. Macintosh systems were successful in education and desktop publishing, making Apple the second-largest PC manufacturer for the next decade. In the early 1990s, Apple introduced the Macintosh LC II and Color Classic which were price-competitive with Wintel machines at the time.
    However, the introduction of Windows 3.1 and Intel's Pentium processor, which beat the Motorola 68040 used in then-current Macintoshes in most benchmarks, gradually took market share from Apple, and by the end of 1994 Apple was relegated to third place as Compaq became the top PC manufacturer. Even after the transition to the superior PowerPC-based Power Macintosh line in the mid-1990s, the falling prices of commodity PC components, poor inventory management with the Macintosh Performa, and the release of Windows 95 contributed to continued decline of the Macintosh user base.
        Upon his return to the company, Steve Jobs led Apple to consolidate the complex line of nearly twenty Macintosh models in mid-1997 (including models made for specific regions) down to four in mid-1999: the Power Macintosh G3, iMac, 14.1" PowerBook G3, and 12" iBook.All four products were critically and commercially successful due to their high performance, competitive prices, and aesthetic designs, and helped return Apple to profitability.
        Around this time, Apple phased out the Macintosh name in favor of "Mac", a nickname that had been in common use since the development of the first model. After their transition to Intel processors in 2006, the complete lineup was Intel-based. This changed in 2020 when the M1 chip was introduced to the MacBook Air, entry level MacBook Pro and Mac mini.</p>
</p>

  <color:Green>
  <h2>The World Wide Web</h2>
</color:Green>

  <p>The world Wide Webarly <i> World Wide Web</i> (WWW), commonly known as the Web, is an place where system documents and other web resources are identified stored accessible over the Internet.[1][2] The resources of the Web are moved via the Hypertext Transfer Protocol (HTTP), may be accessed by users by a software application called a web browser, and are published by a software application called a web server.The World Wide Web is not synonymous with the Internet, which pre-dated the Web in some form by over two decades and upon which technologies the Web is built.
    English scientist Sir Timothy Berners-Lee created the World Wide Web in 1989. He wrote the first web browser in 1990 while employed at CERN near Geneva, Switzerland.[3][4] The browser was released outside CERN to other research institutions starting in January 1991, and then to the public in August 1991. The Web began to enter everyday use in 1993-4, when websites for general use started to become available.[5] The World Wide Web has been central to the development of the Information Age, and is the primary tool billions of people use to interact on the Internet.[6][7][8][9][10]
          Web resources may be any type of downloaded media, but web pages are hypertext documents formatted in Hypertext Markup Language (HTML).[11] Special HTML syntax displays embedded hyperlinks with URLs which permits users to navigate to other web resources. In addition to text, web pages may contain references to images, video, audio, and software components which are either displayed or internally executed in the user's web browser to render pages or streams of multimedia content.
  
  <color:purple>
  <h2><i> Broadban internet</i>
        </color:purple></h2>
         <img src="https://www.google.com/imgres?imgurl=https%3A%2F%2Fhi-techip.com%2Fwp-content%2Fuploads%2F2019%2F06%2FInternet-Broadband-780x405.jpg&imgrefurl=https%3A%2F%2Fhi-techip.com%2Finternet-broadband-how-to-pick-the-very-best-service%2F&tbnid=cg9bF7rhIeCzGM&vet=12ahUKEwjPk5ii98fuAhURBlMKHb2ID6sQMygIegUIARDcAQ..i&docid=O4ED1t9bb6SJqM&w=780&h=405&q=Broadband%20internet&ved=2ahUKEwjPk5ii98fuAhURBlMKHb2ID6sQMygIegUIARDcAQ" alt=" height:200px">
  <p>Different criteria applied in different contexts and at different times. Its origin is in physics, acoustics, and radio systems engineering, where it had been used with a meaning fiudio band system design of the compander. Later, with the advent of digital telecommunications, the term was mainly used for transmission over multiple channels. Whereas a passband signal is also modulated so that it occupies higher frequencies (compared to a baseband signal which is bound to the lowest end of the spectrum, see line coding), it is still occupying a single channel. The key difference is that what is typically considered a broadband signal in this sense is a signal that occupies multiple (non-masking, orthogonal) passbands, thus allowing for much higher throughput over a single medium but with additional complexity in the transmitter/receiver circuitry.

    The term became popularized through the 1990s as a marketing term for Internet access that was faster than dialup access, the original Internet access technology, which was limited to a maximum bandwidth of 56 kbit/s. This meaning is only distantly related to its original technical meaning.
    In telecommunications, a broadband signalling method is one that handles a wide band of frequencies. "Broadband" is a relative term, understood according to its context. The wider (or broader) the bandwidth of a channel, the greater the data-carrying capacity, given the same channel quality.
    
    In radio, for example, a very narrow band will carry Morse code, a broader band will carry speech, and a still broader band will carry music without losing the high audio frequencies required for realistic sound reproduction. This broad band is often divided into channels or "frequency bins" using passband techniques to allow frequency-division multiplexing instead of sending a higher-quality signal.
  </p>

  <color:Black>
  <h2><i>Connected living</i></h2>
</color:Black>
  <P> The 21st century term "<i>Connected living</i>" means that your p, docter apointments and all. "The goal of LivingConnected is to help patients manage their diabetes in real-time, which helps prevent complications and improve their quality of life. Results of live readings can be shared with others, allowing immediate health coaching from Registered Nurses and Certified Diabetes Care & Education Specialists. Ongoing reports and education are also part of the coordination of the patient's care.  "devices are connected to your home able to function without you being present. This is a new turn that has not been used before, 20 years ago the idea of that would be insane, but now with our modern tech fiction has become a reality. The term "Connected living" means that you are connected with the devices in your home about to turn on ligths, fans,TV's, and anything else with just the click of a button. This was created in the 2010's not just to benefit for lazy people but to help as well                          
      Many people that are eldarly or sick people to remind them to take their meds, what time to eat, slee


  </P>

  <color:Red>
  <h2><i>ENIAC</i></h2>
</color:Red>
      
          <imgc="https://www.google.com/imgres?imgurl=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fe%2Fe9%2FENIAC%252C_Ft._Sill%252C_OK%252C_US_%252878%2529.jpg&imgrefurl=https%3A%2F%2Fcommons.wikimedia.org%2Fwiki%2FFile%3AENIAC%2C_Ft._Sill%2C_OK%2C_US_(78).jpg&tbnid=OEo0zFHCGfVFsM&vet=12ahUKEwiOhYGO2cjuAhWL11MKHQX9BMoQMygBegUIARCGAg..i&docid=_6m_OVpOepdhSM&w=6016&h=4016&q=eniac&hl=en&ved=2ahUKEwiOhYGO2cjuAhWL11MKHQX9BMoQMygBegUIARCGAg" alt="height:200px">
  <p>The <i>ENIAC</i> (Electronic Numerical Integrator And Computer), was built between 1943 and 1945—the first large-scale computer to run at electronic speed without being slowed by any mechanical parts. For a decade, until a 1955 lightning strike, ENIAC may have run more calculations than all mankind had done up to that point.ENIAC's design and construction was financed by the United States Army, Ordnance Corps, Research and Development Command, led by Major General Gladeon M. Barnes. The total cost was about $487,000, equivalent to $7,195,000 in 2019. The construction contract was signed on June 5, 1943; work on the computer began in secret at the University of Pennsylvania's Moore School of Electrical Engineering[14] the following month, under the code name "Project PX", with John Grist Brainerd as principal investigator. Herman H. Goldstine persuaded the Army to fund the project, which put him in charge to oversee it for them.
  <i>ENIAC</i> was designed by John Mauchly and J. Presper Eckert of the University of Pennsylvania, U.S.The team of design engineers assisting the development included Robert F. Shaw (function tables), Jeffrey Chuan Chu (divider/square-rooter), Thomas Kite Sharpless (master programmer), Frank Mural (master programmer), Arthur Burks (multiplier), Harry Huskey (reader/printer) and Jack Davis (accumulators).[17] Significant development work was undertaken by the ENIAC women programmers: Jean Jennings, Marlyn Wescoff, Ruth Lichterman, Betty Snyder, Frances Bilas, and Kay McNulty. In 1946, the researchers resigned from the University of Pennsylvania and formed the Eckert-Mauchly Computer Corporation.
  ENIAC's design and construction was financed by the United States Army, Ordnance Corps, Research and Development Command, led by Major General Gladeon M. Barnes. The total cost was about $487,000, equivalent to $7,195,000 in 2019.[13] The construction contract was signed on June 5, 1943; work on the computer began in secret at the University of Pennsylvania's Moore School of Electrical Engineering[14] the following month, under the code name "Project PX", with John Grist Brainerd as principal investigator. Herman H. Goldstine persuaded the Army to fund the project, which put him in charge to oversee it for them.  </p>
</body>
<html>  
